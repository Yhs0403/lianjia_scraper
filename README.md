# 系统设计文档
## 1. 概述
该系统是一个用于爬取二手房信息并存储到MySQL数据库中的Python应用。系统访问链家网（如南京市的二手房页面），通过构造和发送HTTP请求来获取页面信息，并将解析后的房屋信息存储到本地数据库中。



## 2. 系统架构
系统主要包含以下模块：



爬虫模块：用于从目标网站获取房屋信息，包括房屋的名称、描述、小区信息、价格等。

数据处理模块：对抓取到的数据进行清洗和结构化处理，以便存储到数据库中。

数据库模块：负责与MySQL数据库进行交互，创建表结构并将抓取的数据插入表中。

日志模块：记录程序执行过程中的日志，跟踪数据获取进度和错误处理。



## 3. 主要技术栈
语言：Python 3.x

库/框架：

requests：用于发送HTTP请求。

BeautifulSoup：用于解析HTML页面，提取有效信息。

pymysql：用于连接并操作MySQL数据库。

time、random：用于控制爬虫请求间隔，避免过度请求导致被封。

## 4. 数据库设计
表名：ljesf

字段结构：



house_code：房屋编号（字符串）

house_url：房屋链接（字符串）

house_name：房屋名称（字符串）

house_desc：房屋描述（字符串）

xiaoqu_info：小区信息（字符串）

house_tag：房屋标签（字符串）

house_total_price：总价（字符串）

house_unit_price：单价（字符串）

city：城市名称（字符串）

## 5. 流程设计
发送初始请求以获取城市页面并初始化会话。

根据分页结构，逐页获取二手房信息，处理并提取有效数据。

将数据存储到MySQL数据库中。

记录日志，监控每个步骤的执行状态。

# 使用文档
## 1. 安装步骤
确保已经安装Python 3.x版本。

安装项目依赖库：

pip install requests beautifulsoup4 pymysql

确保MySQL数据库已启动，并创建一个数据库（如：lianjia）。

## 2. 修改配置
在脚本中修改get_house_info函数的city_url和city_name，可以指定需要爬取的城市链接。

修改create_table_mysql函数的host、password、database等参数，确保数据库连接信息正确。

## 3. 运行步骤
在终端中运行以下命令：

python lianjia_scraper.py

程序将开始爬取数据，并将房屋信息插入MySQL数据库。



## 4. 常见问题
连接超时或页面无法访问：可能是请求频率过高导致IP被封，建议增加请求间隔或使用代理。

数据库连接失败：检查MySQL服务是否开启，账号密码是否正确，或者防火墙设置是否阻止了访问。



# 测试文档
## 1. 测试环境
操作系统：Ubuntu 20.04 或 Windows 10

MySQL 版本：5.7 或更高

Python 版本：3.8 或更高

## 2. 测试用例
编号       测试内容       预期结果       实际结果       备注

TC001    爬取南京市链家二手房数据      成功抓取房屋信息并存储到数据库   通过       

TC002    页面返回状态非200    自动跳过并输出日志信息   通过       

TC003    MySQL数据库插入错误     回滚事务，保证数据一致性      通过       

TC004    数据库连接失败   报错并停止程序运行   通过       



## 3. 异常处理测试
模拟网络超时，观察程序是否自动重试或退出。通过

测试MySQL服务不可用时，程序是否正确处理数据库连接错误。通过

## 4. 性能测试
并发测试：使用多个不同城市的URL，测试系统在并发情况下的数据抓取能力。通过

大数据测试：爬取多个城市，测试数据量较大时插入数据库的性能表现。通过

